{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Genre Classification Project\n",
    "\n",
    "This notebook will use various Python libraries to perform build a machine learning model using audio file metadata.\n",
    "\n",
    "## 1. Problem definition\n",
    "> Given features extracted from an audio file, can we predict the genre that the audio belongs to\n",
    "\n",
    "## 2. Data\n",
    "The data came from the Free Music Archive developed by several people at École Polytechnique Fédérale de Lausann (EPFL) and Nanyang Technological University (NTU). \n",
    "The research paper can be found [here](https://arxiv.org/pdf/1612.01840.pdf) and the GitHub repository containing the project files can be found [here](https://github.com/mdeff/fma).\n",
    "\n",
    "## 3. Evaluation\n",
    "> If we can reach accuracy that is above the highest benchmark recorded in the research paper, the project is complete.\n",
    "\n",
    "Here is the benchmarks taken from the FMA research paper:\n",
    "\n",
    "| Feature set          | LR | kNN | SVM | MLP |\n",
    "|----------------------|----|-----|-----|-----|\n",
    "| 1 Chroma             | 44 | 44  | 48  | 49  |\n",
    "| 2 Tonnetz            | 40 | 37  | 42  | 41  |\n",
    "| 3 MFCC               | 58 | 55  | 61  | 53  |\n",
    "| 4 Spec. centroid     | 42 | 45  | 46  | 48  |\n",
    "| 5 Spec. bandwidth    | 41 | 45  | 44  | 45  |\n",
    "| 6 Spec. contrast     | 51 | 50  | 54  | 53  |\n",
    "| 7 Spec. rolloff      | 42 | 46  | 48  | 48  |\n",
    "| 8 RMS energy         | 37 | 39  | 39  | 39  |\n",
    "| 9 Zero-crossing rate | 42 | 45  | 45  | 46  |\n",
    "| 3 + 6                | 60 | 55  | 63  | 54  |\n",
    "| 3 + 6 + 4            | 60 | 55  | 63  | 53  |\n",
    "| 1 to 9               | 61 | 52  | 63  | 58  |\n",
    "\n",
    "The values in the table represent the accuracy % of a feature set and a given model.\n",
    "\n",
    "The models are defined as:\n",
    "* LR = Linear Regression with an _L<sup>2</sup>_ penalty\n",
    "* kNN = k-nearest neighbours with k = 200\n",
    "* SVM = support vector machines (SVM) with a radial basis function (RBF)\n",
    "* MLP = multilayer perceptron (MLP) with 100 hidden neurons\n",
    "\n",
    "## 4. Features\n",
    "Here are the features that come with each audio file:\n",
    "- [Chroma](https://en.wikipedia.org/wiki/Chroma_feature)\n",
    "- [Tonnetz](https://en.wikipedia.org/wiki/Tonnetz)\n",
    "- [Mel-frequency cepstral coefficients (MFCC)](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum)\n",
    "- [Spectral centroid](https://en.wikipedia.org/wiki/Centroid)\n",
    "- [Spectral bandwidth](https://en.wikipedia.org/wiki/Bit_rate)\n",
    "- [Spectral contrast](https://www.researchgate.net/publication/3968978_Music_type_classification_by_spectral_contrast_feature)\n",
    "- [Spectral rolloff](https://www.dsprelated.com/freebooks/sasp/Spectral_Roll_Off.html)\n",
    "- [RMS energy](http://replaygain.hydrogenaud.io/proposal/rms_energy.html)\n",
    "- [Zero crossing-rate](https://en.wikipedia.org/wiki/Zero-crossing_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the tools\n",
    "\n",
    "We will import all of the Python libraries we will use inside our project, including NumPy, Pandas, Matplotlib, and several Scikit-Learn ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "We will load the data using the `utils.py` utility file provided by the FMA repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = utils.load(\"data/fma_metadata/features.csv\")\n",
    "tracks = utils.load(\"data/fma_metadata/tracks.csv\")\n",
    "echonest = utils.load('data/fma_metadata/echonest.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing data\n",
    "\n",
    "We will do our model testing with the small subset. Let us collect our set of training, validation, and testing data based on what was set from the FMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_subset = tracks.index[tracks.set.subset == 'small']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_tracks = tracks.loc[small_subset]\n",
    "small_features = features.loc[small_subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = small_tracks.index[small_tracks['set', 'split'] == 'training']\n",
    "val = small_tracks.index[small_tracks['set', 'split'] == 'validation']\n",
    "test = small_tracks.index[small_tracks['set', 'split'] == 'test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a helper function to collect the training, validation, and testing set for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_test_data(tracks, features, columns):\n",
    "    \"\"\"\n",
    "    Generate training, validation, and testing data given `tracks` and `features`.\n",
    "    \"\"\"\n",
    "    enc = LabelEncoder()\n",
    "    labels = tracks.track.genre_top\n",
    "    \n",
    "    # Split into train, val, test\n",
    "    X_train = features.loc[train, columns].values\n",
    "    X_val = features.loc[val, columns].values\n",
    "    X_test = features.loc[test, columns].values\n",
    "    y_train = enc.fit_transform(labels[train])\n",
    "    y_val = enc.transform(labels[val])\n",
    "    y_test = enc.transform(labels[test])\n",
    "        \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler(copy=False)\n",
    "    scaler.fit_transform(X_train)\n",
    "    scaler.transform(X_val)\n",
    "    scaler.transform(X_test)\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "Let's check which combination of features work best for testing. We will use the kNN model as our baseline model to make this determination (since to me the time it takes to train this model is very quick)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get combination of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chroma_cens': 'chroma_cens',\n",
       " 'chroma_cqt': 'chroma_cqt',\n",
       " 'chroma_stft': 'chroma_stft',\n",
       " 'mfcc': 'mfcc',\n",
       " 'rmse': 'rmse',\n",
       " 'spectral_bandwidth': 'spectral_bandwidth',\n",
       " 'spectral_centroid': 'spectral_centroid',\n",
       " 'spectral_contrast': 'spectral_contrast',\n",
       " 'spectral_rolloff': 'spectral_rolloff',\n",
       " 'tonnetz': 'tonnetz',\n",
       " 'zcr': 'zcr',\n",
       " 'mfcc/contrast': ['mfcc', 'spectral_contrast'],\n",
       " 'mfcc/contrast/chroma': ['mfcc', 'spectral_contrast', 'chroma_cens'],\n",
       " 'mfcc/contrast/centroid': ['mfcc', 'spectral_contrast', 'spectral_centroid'],\n",
       " 'mfcc/contrast/chroma/centroid': ['mfcc',\n",
       "  'spectral_contrast',\n",
       "  'chroma_cens',\n",
       "  'spectral_centroid'],\n",
       " 'mfcc/contrast/chroma/centroid/tonnetz': ['mfcc',\n",
       "  'spectral_contrast',\n",
       "  'chroma_cens',\n",
       "  'spectral_centroid',\n",
       "  'tonnetz'],\n",
       " 'mfcc/contrast/chroma/centroid/zcr': ['mfcc',\n",
       "  'spectral_contrast',\n",
       "  'chroma_cens',\n",
       "  'spectral_centroid',\n",
       "  'zcr'],\n",
       " 'all_non-echonest': ['chroma_cens',\n",
       "  'chroma_cqt',\n",
       "  'chroma_stft',\n",
       "  'mfcc',\n",
       "  'rmse',\n",
       "  'spectral_bandwidth',\n",
       "  'spectral_centroid',\n",
       "  'spectral_contrast',\n",
       "  'spectral_rolloff',\n",
       "  'tonnetz',\n",
       "  'zcr']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get list of all combinations of features\n",
    "feature_sets = {}\n",
    "\n",
    "for name in small_features.columns.levels[0]:\n",
    "    feature_sets[name] = name\n",
    "\n",
    "feature_sets.update({\n",
    "    'mfcc/contrast': ['mfcc', 'spectral_contrast'],\n",
    "    'mfcc/contrast/chroma': ['mfcc', 'spectral_contrast', 'chroma_cens'],\n",
    "    'mfcc/contrast/centroid': ['mfcc', 'spectral_contrast', 'spectral_centroid'],\n",
    "    'mfcc/contrast/chroma/centroid': ['mfcc', 'spectral_contrast', 'chroma_cens', 'spectral_centroid'],\n",
    "    'mfcc/contrast/chroma/centroid/tonnetz': ['mfcc', 'spectral_contrast', 'chroma_cens', 'spectral_centroid', 'tonnetz'],\n",
    "    'mfcc/contrast/chroma/centroid/zcr': ['mfcc', 'spectral_contrast', 'chroma_cens', 'spectral_centroid', 'zcr'],\n",
    "    'all_non-echonest': list(features.columns.levels[0])\n",
    "})\n",
    "\n",
    "feature_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "model = KNeighborsClassifier(n_neighbors=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test different combination of features\n",
    "\n",
    "We will use `cross_val_score` to have better clarity on which feature performs the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeaturesAndLabelsGivenColumns(columns):\n",
    "    enc = LabelEncoder()\n",
    "    labels = small_tracks.track.genre_top\n",
    "\n",
    "    X = small_features[columns].values\n",
    "    y = enc.fit_transform(labels)\n",
    "\n",
    "    scaler = StandardScaler(copy=False)\n",
    "    scaler.fit_transform(X)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: chroma_cens, Accuracy: 0.25%\n",
      "Feature: chroma_cqt, Accuracy: 0.27%\n",
      "Feature: chroma_stft, Accuracy: 0.28%\n",
      "Feature: mfcc, Accuracy: 0.41%\n",
      "Feature: rmse, Accuracy: 0.24%\n",
      "Feature: spectral_bandwidth, Accuracy: 0.31%\n",
      "Feature: spectral_centroid, Accuracy: 0.32%\n",
      "Feature: spectral_contrast, Accuracy: 0.35%\n",
      "Feature: spectral_rolloff, Accuracy: 0.32%\n",
      "Feature: tonnetz, Accuracy: 0.26%\n",
      "Feature: zcr, Accuracy: 0.30%\n",
      "Feature: mfcc/contrast, Accuracy: 0.42%\n",
      "Feature: mfcc/contrast/chroma, Accuracy: 0.40%\n",
      "Feature: mfcc/contrast/centroid, Accuracy: 0.42%\n",
      "Feature: mfcc/contrast/chroma/centroid, Accuracy: 0.40%\n",
      "Feature: mfcc/contrast/chroma/centroid/tonnetz, Accuracy: 0.39%\n",
      "Feature: mfcc/contrast/chroma/centroid/zcr, Accuracy: 0.39%\n",
      "Feature: all_non-echonest, Accuracy: 0.38%\n"
     ]
    }
   ],
   "source": [
    "for f_name, f_set in feature_sets.items():\n",
    "    X, y = getFeaturesAndLabelsGivenColumns(f_set)\n",
    "    cv_acc = cross_val_score(model, X, y, cv=5, scoring=\"accuracy\")\n",
    "    score = np.mean(cv_acc)\n",
    "    print(f\"Feature: {f_name}, Accuracy: {score:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, we have two feature combinations that provide the highest accuracy: `[\"mfcc\", \"spectral_contrast\"]` and `[\"mfcc\", \"spectral_contrast\", \"spectral_centroid\"]`. For now we will use `mfcc/contrast` but we will further test with adding `spectral_centroid` to see the performance results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test different models\n",
    "\n",
    "Now we will test different models on the features to see which model give the best performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: GaussianNB, Accuracy: 0.4085\n",
      "Model: RandomForestClassifier, Accuracy: 0.499625\n",
      "Model: kNN, Accuracy: 0.41987499999999994\n",
      "Model: SVC-linear, Accuracy: 0.496125\n",
      "Model: SVC-poly, Accuracy: 0.518375\n",
      "Model: SVC-rbf, Accuracy: 0.536875\n",
      "Model: MLP, Accuracy: 0.460375\n",
      "Model: AdaBoost, Accuracy: 0.372375\n"
     ]
    }
   ],
   "source": [
    "ml_models = {\n",
    "    \"GaussianNB\": GaussianNB(),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(),\n",
    "    \"kNN\": KNeighborsClassifier(n_neighbors=200),\n",
    "    \"SVC-linear\": SVC(kernel=\"linear\"),\n",
    "    \"SVC-poly\": SVC(kernel=\"poly\", degree=1),\n",
    "    \"SVC-rbf\": SVC(kernel=\"rbf\"),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), max_iter=2000),\n",
    "    \"AdaBoost\": AdaBoostClassifier(n_estimators=10)\n",
    "}\n",
    "\n",
    "fset = [\"mfcc\", \"spectral_contrast\"]\n",
    "X, y = getFeaturesAndLabelsGivenColumns(fset)\n",
    "\n",
    "for model_name, ml_model in ml_models.items():\n",
    "    cv_acc = cross_val_score(ml_model, X, y, cv=5, scoring=\"accuracy\")\n",
    "    score = np.mean(cv_acc)\n",
    "    print(f\"Model: {model_name}, Accuracy: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the benchmark results above, it seems that our SVC model with radial basis functions performed the best with a 53.7% prediction rate! We will be selecting this model for performance.\n",
    "\n",
    "Now we have our model, how can we improve our performance accuracy? We should perform hyperparameter tuning to see what different settings different results bring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "\n",
    "According to [scikit-learn.org](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html), there are two parameters that can be tuned for our SVC model: `gamma` and `C`. Since our parameter choices are small, we can perform `GridSearchCV` to figure out which parameters performs the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set our model to SVC\n",
    "np.random.seed(42)\n",
    "model = SVC(kernel=\"rbf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    \"C\": [10**-4, 10**-2, 10**0, 10**2, 10**4],\n",
    "    \"gamma\": [10**-2, 10**-1, 10**0, 10**1, 10**2]\n",
    "}\n",
    "\n",
    "# Setup GridSearchCV\n",
    "tuned_model = GridSearchCV(estimator=model, \n",
    "                            param_grid=grid,\n",
    "                           n_jobs=10, \n",
    "                           cv=5,\n",
    "                           verbose=2)\n",
    "\n",
    "# Prepare our test data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = get_train_val_test_data(small_tracks, small_features, fset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
      "[Parallel(n_jobs=10)]: Done  21 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=10)]: Done 125 out of 125 | elapsed:  5.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=SVC(C=10, gamma=0.1), n_jobs=10,\n",
       "             param_grid={'C': [0.0001, 0.01, 1, 100, 10000],\n",
       "                         'gamma': [0.01, 0.1, 1, 10, 100]},\n",
       "             verbose=2)"
      ]
     },
     "execution_count": 382,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to see which parameters returned the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'gamma': 0.01}"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_model.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've tuned the model and got these params, let's check to see if we actually got a better result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel=\"rbf\", C=1.0, gamma=0.01)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = get_train_val_test_data(small_tracks, small_features, fset)\n",
    "model.fit(X_train, y_train)\n",
    "score = model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47625"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, tuning the hyperparameters returned a score _much_ less than the default score. In this case, it doesn't seem like changing the hyperparameters gives us a better result. \n",
    "\n",
    "Another suggestion we can do is tune the parameters for the next best model: SVC with `kernel=poly`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = {\n",
    "    \"degree\": [1, 2, 3, 5],\n",
    "}\n",
    "\n",
    "model = SVC(kernel=\"poly\")\n",
    "# Setup GridSearchCV\n",
    "tuned_model = RandomizedSearchCV(estimator=model, \n",
    "                            param_distributions=grid,\n",
    "                           n_iter=10, \n",
    "                           cv=5,\n",
    "                           verbose=2)\n",
    "\n",
    "# Prepare our test data\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = get_train_val_test_data(small_tracks, small_features, fset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] gamma=100, degree=1, C=10000 ....................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "tuned_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'degree': 1}"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel=\"poly\", degree=1)\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = get_train_val_test_data(small_tracks, small_features, fset)\n",
    "model.fit(X_train, y_train)\n",
    "score = model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41875"
      ]
     },
     "execution_count": 398,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid = {\"n_estimators\": [10, 100, 200, 500, 1000, 1200],\n",
    "       \"max_depth\": [None, 5, 10, 20, 30],\n",
    "       \"max_features\": [\"auto\", \"sqrt\"],\n",
    "       \"min_samples_split\": [2, 4, 6],\n",
    "       \"min_samples_leaf\": [1, 2, 4]}\n",
    "\n",
    "fset = [\"mfcc\", \"spectral_contrast\"]\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "# Setup RandomizedSearchCV\n",
    "tuned_model = RandomizedSearchCV(estimator=model, \n",
    "                            param_distributions=grid,\n",
    "                           n_iter=10, # num of models to try\n",
    "                           cv=5,\n",
    "                           verbose=2)\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = get_train_val_test_data(small_tracks, small_features, fset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] n_estimators=1000, min_samples_split=6, min_samples_leaf=4, max_features=sqrt, max_depth=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=1000, min_samples_split=6, min_samples_leaf=4, max_features=sqrt, max_depth=5, total=  19.2s\n",
      "[CV] n_estimators=1000, min_samples_split=6, min_samples_leaf=4, max_features=sqrt, max_depth=5 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   19.2s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=1000, min_samples_split=6, min_samples_leaf=4, max_features=sqrt, max_depth=5, total=  19.2s\n",
      "[CV] n_estimators=1000, min_samples_split=6, min_samples_leaf=4, max_features=sqrt, max_depth=5 \n",
      "[CV]  n_estimators=1000, min_samples_split=6, min_samples_leaf=4, max_features=sqrt, max_depth=5, total=  18.9s\n",
      "[CV] n_estimators=1000, min_samples_split=6, min_samples_leaf=4, max_features=sqrt, max_depth=5 \n",
      "[CV]  n_estimators=1000, min_samples_split=6, min_samples_leaf=4, max_features=sqrt, max_depth=5, total=  19.0s\n",
      "[CV] n_estimators=1000, min_samples_split=6, min_samples_leaf=4, max_features=sqrt, max_depth=5 \n",
      "[CV]  n_estimators=1000, min_samples_split=6, min_samples_leaf=4, max_features=sqrt, max_depth=5, total=  18.9s\n",
      "[CV] n_estimators=500, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=30 \n",
      "[CV]  n_estimators=500, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=30, total=  18.9s\n",
      "[CV] n_estimators=500, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=30 \n",
      "[CV]  n_estimators=500, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=30, total=  18.8s\n",
      "[CV] n_estimators=500, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=30 \n",
      "[CV]  n_estimators=500, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=30, total=  18.9s\n",
      "[CV] n_estimators=500, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=30 \n",
      "[CV]  n_estimators=500, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=30, total=  18.7s\n",
      "[CV] n_estimators=500, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=30 \n",
      "[CV]  n_estimators=500, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=30, total=  18.7s\n",
      "[CV] n_estimators=500, min_samples_split=2, min_samples_leaf=4, max_features=sqrt, max_depth=30 \n",
      "[CV]  n_estimators=500, min_samples_split=2, min_samples_leaf=4, max_features=sqrt, max_depth=30, total=  17.3s\n",
      "[CV] n_estimators=500, min_samples_split=2, min_samples_leaf=4, max_features=sqrt, max_depth=30 \n",
      "[CV]  n_estimators=500, min_samples_split=2, min_samples_leaf=4, max_features=sqrt, max_depth=30, total=  17.1s\n",
      "[CV] n_estimators=500, min_samples_split=2, min_samples_leaf=4, max_features=sqrt, max_depth=30 \n",
      "[CV]  n_estimators=500, min_samples_split=2, min_samples_leaf=4, max_features=sqrt, max_depth=30, total=  17.1s\n",
      "[CV] n_estimators=500, min_samples_split=2, min_samples_leaf=4, max_features=sqrt, max_depth=30 \n",
      "[CV]  n_estimators=500, min_samples_split=2, min_samples_leaf=4, max_features=sqrt, max_depth=30, total=  17.0s\n",
      "[CV] n_estimators=500, min_samples_split=2, min_samples_leaf=4, max_features=sqrt, max_depth=30 \n",
      "[CV]  n_estimators=500, min_samples_split=2, min_samples_leaf=4, max_features=sqrt, max_depth=30, total=  17.1s\n",
      "[CV] n_estimators=1000, min_samples_split=4, min_samples_leaf=2, max_features=sqrt, max_depth=5 \n",
      "[CV]  n_estimators=1000, min_samples_split=4, min_samples_leaf=2, max_features=sqrt, max_depth=5, total=  18.8s\n",
      "[CV] n_estimators=1000, min_samples_split=4, min_samples_leaf=2, max_features=sqrt, max_depth=5 \n",
      "[CV]  n_estimators=1000, min_samples_split=4, min_samples_leaf=2, max_features=sqrt, max_depth=5, total=  18.8s\n",
      "[CV] n_estimators=1000, min_samples_split=4, min_samples_leaf=2, max_features=sqrt, max_depth=5 \n",
      "[CV]  n_estimators=1000, min_samples_split=4, min_samples_leaf=2, max_features=sqrt, max_depth=5, total=  18.8s\n",
      "[CV] n_estimators=1000, min_samples_split=4, min_samples_leaf=2, max_features=sqrt, max_depth=5 \n",
      "[CV]  n_estimators=1000, min_samples_split=4, min_samples_leaf=2, max_features=sqrt, max_depth=5, total=  18.9s\n",
      "[CV] n_estimators=1000, min_samples_split=4, min_samples_leaf=2, max_features=sqrt, max_depth=5 \n",
      "[CV]  n_estimators=1000, min_samples_split=4, min_samples_leaf=2, max_features=sqrt, max_depth=5, total=  18.9s\n",
      "[CV] n_estimators=1200, min_samples_split=4, min_samples_leaf=2, max_features=sqrt, max_depth=20 \n",
      "[CV]  n_estimators=1200, min_samples_split=4, min_samples_leaf=2, max_features=sqrt, max_depth=20, total=  44.9s\n",
      "[CV] n_estimators=1200, min_samples_split=4, min_samples_leaf=2, max_features=sqrt, max_depth=20 \n",
      "[CV]  n_estimators=1200, min_samples_split=4, min_samples_leaf=2, max_features=sqrt, max_depth=20, total=  44.7s\n",
      "[CV] n_estimators=1200, min_samples_split=4, min_samples_leaf=2, max_features=sqrt, max_depth=20 \n",
      "[CV]  n_estimators=1200, min_samples_split=4, min_samples_leaf=2, max_features=sqrt, max_depth=20, total=  45.0s\n",
      "[CV] n_estimators=1200, min_samples_split=4, min_samples_leaf=2, max_features=sqrt, max_depth=20 \n",
      "[CV]  n_estimators=1200, min_samples_split=4, min_samples_leaf=2, max_features=sqrt, max_depth=20, total=  44.4s\n",
      "[CV] n_estimators=1200, min_samples_split=4, min_samples_leaf=2, max_features=sqrt, max_depth=20 \n",
      "[CV]  n_estimators=1200, min_samples_split=4, min_samples_leaf=2, max_features=sqrt, max_depth=20, total=  45.0s\n",
      "[CV] n_estimators=200, min_samples_split=6, min_samples_leaf=4, max_features=sqrt, max_depth=30 \n",
      "[CV]  n_estimators=200, min_samples_split=6, min_samples_leaf=4, max_features=sqrt, max_depth=30, total=   6.9s\n",
      "[CV] n_estimators=200, min_samples_split=6, min_samples_leaf=4, max_features=sqrt, max_depth=30 \n",
      "[CV]  n_estimators=200, min_samples_split=6, min_samples_leaf=4, max_features=sqrt, max_depth=30, total=   7.0s\n",
      "[CV] n_estimators=200, min_samples_split=6, min_samples_leaf=4, max_features=sqrt, max_depth=30 \n",
      "[CV]  n_estimators=200, min_samples_split=6, min_samples_leaf=4, max_features=sqrt, max_depth=30, total=   7.2s\n",
      "[CV] n_estimators=200, min_samples_split=6, min_samples_leaf=4, max_features=sqrt, max_depth=30 \n",
      "[CV]  n_estimators=200, min_samples_split=6, min_samples_leaf=4, max_features=sqrt, max_depth=30, total=   7.1s\n",
      "[CV] n_estimators=200, min_samples_split=6, min_samples_leaf=4, max_features=sqrt, max_depth=30 \n",
      "[CV]  n_estimators=200, min_samples_split=6, min_samples_leaf=4, max_features=sqrt, max_depth=30, total=   7.2s\n",
      "[CV] n_estimators=1000, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=20 \n",
      "[CV]  n_estimators=1000, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=20, total=  39.4s\n",
      "[CV] n_estimators=1000, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=20 \n",
      "[CV]  n_estimators=1000, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=20, total=  38.3s\n",
      "[CV] n_estimators=1000, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=20 \n",
      "[CV]  n_estimators=1000, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=20, total=  38.2s\n",
      "[CV] n_estimators=1000, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=20 \n",
      "[CV]  n_estimators=1000, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=20, total=  38.6s\n",
      "[CV] n_estimators=1000, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=20 \n",
      "[CV]  n_estimators=1000, min_samples_split=2, min_samples_leaf=2, max_features=auto, max_depth=20, total=  38.5s\n",
      "[CV] n_estimators=1000, min_samples_split=6, min_samples_leaf=4, max_features=auto, max_depth=5 \n",
      "[CV]  n_estimators=1000, min_samples_split=6, min_samples_leaf=4, max_features=auto, max_depth=5, total=  19.6s\n",
      "[CV] n_estimators=1000, min_samples_split=6, min_samples_leaf=4, max_features=auto, max_depth=5 \n",
      "[CV]  n_estimators=1000, min_samples_split=6, min_samples_leaf=4, max_features=auto, max_depth=5, total=  19.9s\n",
      "[CV] n_estimators=1000, min_samples_split=6, min_samples_leaf=4, max_features=auto, max_depth=5 \n",
      "[CV]  n_estimators=1000, min_samples_split=6, min_samples_leaf=4, max_features=auto, max_depth=5, total=  19.2s\n",
      "[CV] n_estimators=1000, min_samples_split=6, min_samples_leaf=4, max_features=auto, max_depth=5 \n",
      "[CV]  n_estimators=1000, min_samples_split=6, min_samples_leaf=4, max_features=auto, max_depth=5, total=  19.7s\n",
      "[CV] n_estimators=1000, min_samples_split=6, min_samples_leaf=4, max_features=auto, max_depth=5 \n",
      "[CV]  n_estimators=1000, min_samples_split=6, min_samples_leaf=4, max_features=auto, max_depth=5, total=  19.0s\n",
      "[CV] n_estimators=1000, min_samples_split=4, min_samples_leaf=2, max_features=auto, max_depth=10 \n",
      "[CV]  n_estimators=1000, min_samples_split=4, min_samples_leaf=2, max_features=auto, max_depth=10, total=  31.9s\n",
      "[CV] n_estimators=1000, min_samples_split=4, min_samples_leaf=2, max_features=auto, max_depth=10 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=1000, min_samples_split=4, min_samples_leaf=2, max_features=auto, max_depth=10, total=  31.8s\n",
      "[CV] n_estimators=1000, min_samples_split=4, min_samples_leaf=2, max_features=auto, max_depth=10 \n",
      "[CV]  n_estimators=1000, min_samples_split=4, min_samples_leaf=2, max_features=auto, max_depth=10, total=  31.5s\n",
      "[CV] n_estimators=1000, min_samples_split=4, min_samples_leaf=2, max_features=auto, max_depth=10 \n",
      "[CV]  n_estimators=1000, min_samples_split=4, min_samples_leaf=2, max_features=auto, max_depth=10, total=  31.5s\n",
      "[CV] n_estimators=1000, min_samples_split=4, min_samples_leaf=2, max_features=auto, max_depth=10 \n",
      "[CV]  n_estimators=1000, min_samples_split=4, min_samples_leaf=2, max_features=auto, max_depth=10, total=  31.3s\n",
      "[CV] n_estimators=500, min_samples_split=6, min_samples_leaf=1, max_features=auto, max_depth=5 \n",
      "[CV]  n_estimators=500, min_samples_split=6, min_samples_leaf=1, max_features=auto, max_depth=5, total=   9.3s\n",
      "[CV] n_estimators=500, min_samples_split=6, min_samples_leaf=1, max_features=auto, max_depth=5 \n",
      "[CV]  n_estimators=500, min_samples_split=6, min_samples_leaf=1, max_features=auto, max_depth=5, total=   9.4s\n",
      "[CV] n_estimators=500, min_samples_split=6, min_samples_leaf=1, max_features=auto, max_depth=5 \n",
      "[CV]  n_estimators=500, min_samples_split=6, min_samples_leaf=1, max_features=auto, max_depth=5, total=   9.4s\n",
      "[CV] n_estimators=500, min_samples_split=6, min_samples_leaf=1, max_features=auto, max_depth=5 \n",
      "[CV]  n_estimators=500, min_samples_split=6, min_samples_leaf=1, max_features=auto, max_depth=5, total=   9.3s\n",
      "[CV] n_estimators=500, min_samples_split=6, min_samples_leaf=1, max_features=auto, max_depth=5 \n",
      "[CV]  n_estimators=500, min_samples_split=6, min_samples_leaf=1, max_features=auto, max_depth=5, total=   9.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed: 18.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=RandomForestClassifier(),\n",
       "                   param_distributions={'max_depth': [None, 5, 10, 20, 30],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 4, 6],\n",
       "                                        'n_estimators': [10, 100, 200, 500,\n",
       "                                                         1000, 1200]},\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_estimators': 1000,\n",
       " 'min_samples_split': 2,\n",
       " 'min_samples_leaf': 2,\n",
       " 'max_features': 'auto',\n",
       " 'max_depth': 20}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45875"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel=\"rbf\")\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = get_train_val_test_data(small_tracks, small_features, fset)\n",
    "model.fit(X_train, y_train)\n",
    "score = model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47875"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of one of the downfalls of searching for hyperparameters to tune. Sometimes the results come out worse than the default baseline models, and that's okay. The baseline model could be enough to receive the best predictions.\n",
    "\n",
    "Let's add `spectral_centroid` to see if we receive higher accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fset = ['mfcc', 'spectral_contrast', 'spectral_centroid']\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = get_train_val_test_data(small_tracks, small_features, fset)\n",
    "model.fit(X_train, y_train)\n",
    "score = model.score(X_test, y_test)\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Now that we've received our baseline model, we can now do predictions on the medium dataset to compare our benchmarks to the research papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_subset = tracks.index[tracks.set.subset == 'medium']\n",
    "medium_tracks = tracks.loc[medium_subset]\n",
    "medium_features = features.loc[medium_subset]\n",
    "\n",
    "train = medium_tracks.index[medium_tracks['set', 'split'] == 'training']\n",
    "val = medium_tracks.index[medium_tracks['set', 'split'] == 'validation']\n",
    "test = medium_tracks.index[medium_tracks['set', 'split'] == 'test']\n",
    "\n",
    "fset = ['mfcc', 'spectral_contrast', 'spectral_centroid']\n",
    "\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = get_train_val_test_data(medium_tracks, medium_features, fset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(kernel=\"rbf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the data to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7157360406091371"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "We concluded the project with a prediction of 71.6%! According to our table listed above, the largest prediction we gained from the research paper was 63%. \n",
    "\n",
    "**Note:** This is just a quick introductory project to get myself used to testing different features, different models, and different hyperparameters. There's a lot of deeper studying of the dataset to do if I want to achieve better results. However, I will leave this work to another project I plan to move to production. This project is just to showcase my knowledge on the aforementioned subjects and other projects will showcase more focused, productive work.\n",
    "\n",
    "Yanique Andre"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
